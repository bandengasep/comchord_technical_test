Participants:
? Sarah Chen (Manager): Senior Director of Product
? Javier Morales (Direct Report): QA Lead
Date: September 7, 2025
Time: 11:00 AM - 12:00 PM
Format: Video call
Sarah: Hi Javier. Thanks for making the time.
Javier: Hello Sarah. Of course. Everything is good on your end?
Sarah: Yes, everything is fine. Just the usual busy start to the month. How was your weekend?
Javier: It was productive. I was able to complete some tasks around the house. I hope yours was restful.
Sarah: It was, thank you. So, let's jump in. I wanted to start with the high-level metrics. I was looking at the dashboard you send out. Bug escape rate seems to be holding steady, which is good. But I wanted to talk about test coverage.
Javier: Yes. The code coverage metric is at 78 percent for the main application repository. This is a 2 percent increase from the previous quarter, which is positive. The services repository is holding at 84 percent.
Sarah: That's good progress. I guess my question is more are these the right metrics for us to be tracking? Code coverage feels like an engineering metric, not necessarily a a user experience metric. Do we know if that 78 percent is covering the right 78 percent?
Javier: That is a valid question. The metric itself is a quantitative measure of lines of code executed by the automated test suite. It does not inherently measure the qualitative value of those tests.
Sarah: Right. So are we are we thinking about a different way to measure this? Like like coverage of critical user flows?
Javier: We do have that. We have a separate matrix that maps our test cases, both manual and automated, to the primary and secondary user journeys that were defined by the product team in Q1. That is tracked in our test case management system. The report from that system indicates we have 95 percent coverage of P0 user flows and 88 percent coverage of P1 user flows.
Sarah: Okay. So we have two different coverage metrics. The code coverage one and the user flow one.
Javier: Correct. The code coverage metric is for the unit and integration test layer. The user flow metric is for our end-to-end and manual regression suites. They measure different things.
Sarah: I see. It just feels complex. When Alex and I talk, we look at user adoption or engagement. When I look at your dashboard, I see I see a lot of process metrics. Escape rate, test coverage, test pass rate. I'm just I'm wondering if we're missing the the human element. How do we know we're shipping a good product, not just a a product that that matches the specs?
Javier: Our mandate in the Quality Assurance organization is to verify and validate that the software meets the agreed-upon functional and non-functional requirements. The the subjective 'goodness' of the product is that is a product management function, is it not? We we test what is defined in the acceptance criteria. If the acceptance criteria are met, the ticket is passed.
Sarah: I I suppose so. I just I worry that we're all focused on our individual silos. Product writes the ticket, engineering builds the ticket, QA tests the ticket. But but no one is looking at the whole picture.
Javier: I agree that a holistic view is important. That is why we participate in the cross-functional team meetings and backlog grooming. We provide feedback on testability and potential risks based on the requirements presented.
Sarah: Okay. Well let's let's maybe put a pin in that. It's a bigger conversation. I just I want us to be thinking about it.
Javier: Understood. I will add it as a potential discussion point for our next QA team meeting.
Sarah: Great. Let's let's switch gears to automation. This this is the big one. I know you've been working on this for for almost a year now. What's the real status?
Javier: The test automation initiative is proceeding. It is a complex undertaking, as you know. We have significant technical debt in the frontend codebase, which makes many of our UI tests 'flaky' is the common term. They they fail due to timing issues or DOM rendering delays, not due to actual bugs.
Sarah: Right. I hear about flaky tests all the time from the engineering managers. They they say it slows down their CI pipeline.
Javier: That is correct. A flaky test undermines trust in the entire regression suite. If a developer gets a red build, they must spend time investigating if it is a real failure or just another flake. This creates friction.
Sarah: So what are we doing about it?
Javier: We have a multi-pronged strategy. First, we are quarantining the known flaky tests. We run them in a separate job so they do not block the main build. Second, my team is actively refactoring the worst offenders, moving them away from brittle CSS selectors and toward more stable data-testid attributes. This requires collaboration with the frontend engineers, as they must add those attributes to the code.
Sarah: That sounds slow.
Javier: It is methodical. It is the only way to create a stable suite. We we cannot automate on top of a weak foundation. Third, we are shifting our focus left.
Sarah: 'Shifting left'. You you say that a lot. What does that actually what does that mean in practice?
Javier: It means we are investing more heavily in API-level and integration-level tests. These are faster, more stable, and and they can catch bugs before the UI is even built. A single API test can validate business logic that might require say twenty steps in a UI test. We are building our automation at a lower layer of the pyramid.
Sarah: Okay. So we're we're writing fewer UI tests and more API tests.
Javier: In aggregate, yes. We are being more selective about what we automate at the UI layer. Only the most critical, high-level user flows. Everything else logins, data validation, error handling that should be tested at the API or integration layer.
Sarah: That that makes sense. So when? When will this be 'done'? When will we have this this stable, fast automation suite you're describing?
Javier: 'Done' is 'done' is not the correct term for automation. It is a it is an ongoing process of maintenance and improvement, just like the product code itself. It is never 'done'.
Sarah: Okay, Javier. I I understand that. But when will we reach the the milestone? The the point where we can say we have say 80% of our regressions automated? What's the what's the timeline for that?
Javier: The timeline is dependent on several factors. The the refactoring effort from engineering, for one. And the the technical debt I mentioned. We are also evaluating new frameworks.
Sarah: New frameworks? I I thought we chose Cypress. We we spent months on that evaluation.
Javier: We did. And Cypress is excellent for our UI tests. But for API and performance testing, we are we are evaluating other tools. K6, for example, for load testing. And Playwright has shown some some significant advantages in its its handling of parallel execution.
Sarah: So we're we're switching tools? Or or adding more? I I'm confused.
Javier: We are exploring. It is part of our due diligence to ensure we are using the best-in-class tools for the job. I I do not want to commit to a five-year plan with a tool that might be superseded in one year.
Sarah: I I feel like we're having this exact same conversation we had six months ago. I I need a plan, Javier. I I need a a roadmap. A a date I can I can communicate to the other leaders. When can I tell Mike, the VP of Engineering, that our QA pipeline is is green?
Javier: I cannot provide a specific date for that. It is it is not a realistic request. We we are dependent on the stability of the environments, the quality of the code being checked in, and the the flaky test issue. We we can provide our internal roadmap for automation coverage, quarter by quarter. But I I cannot guarantee a green pipeline. No no QA leader could guarantee that.
Sarah: But but that's what we need. We we need confidence to ship faster. The the whole point of this automation investment was was to increase our deployment velocity. And and it feels like it's it's just stalled.
Javier: I would I would disagree with 'stalled'. We have increased our API test suite by 40 percent this quarter. We have reduced the main build failure rate from flaky tests by 60 percent. These are these are significant engineering achievements. They they are just not not as visible as a UI test.
Sarah: I I guess I'll have to take your word for that. Can can you send me that roadmap? The the quarter-by-quarter one?
Javier: Yes. I will email you the link to the Confluence page. It is already documented.
Sarah: Thank you. Okay, let's let's talk about the the P1 bug last week. The the one on the checkout page.
Javier: Yes. The the tax calculation error.
Sarah: That was that was a bad one. It it was subtle, but it it was wrong. And and a user found it, not us. Which is which is what I I really don't like to see. What what happened?
Javier: We completed the Root Cause Analysis on Friday. The the problem was it was a state-based issue. The the user had to apply a coupon code, then change their shipping state from say California to a state with no sales tax, like Oregon, and then go back and change the quantity of the item. This this specific sequence of events triggered a a miscalculation in the the downstream tax microservice.
Sarah: So a an edge case.
Javier: A complex edge case, yes. Our our manual test plan for the checkout flow is it's very robust. It it covers changing states. It covers applying coupons. It covers changing quantities. But it it did not cover that that specific permutation, in that that specific order.
Sarah: So so how do we fix this, going forward? We we can't we can't possibly test for for every single permutation of of clicks, can we?
Javier: No, we cannot. That is that is 'combinatorial explosion'. It it is not feasible. The the fix here is twofold. First, the the immediate fix. We have we have added this exact scenario to our P0 regression suite. It it is now an automated end-to-end test. It it will not happen again.
Sarah: Okay. That's that's reactive. What's what's the the proactive fix?
Javier: The proactive fix is is what I was discussing earlier. This this bug should have been caught at the API level. This this was a flaw in the business logic of the tax service. A a proper contract test between the the frontend and the tax service, or or an integration test within the tax service itself, would have would have found this. The the UI test it's it's the wrong place to catch this kind of bug. It's it's too late.
Sarah: So so this is this is part of the 'shift left' strategy.
Javier: Precisely. This incident is it is a perfect example of why we must invest in API-level automation. It it strengthens our argument.
Sarah: It it just feels like like everything points back to this this big, abstract automation project that that has no end date. It it feels like like the answer to every problem is is 'automation', but but the automation isn't isn't here yet.
Javier: It is it is a journey, Sarah. Not a a destination. We are we are making steady, incremental progress.
Sarah: I I hear you. Okay. Let's let's move on. The the open req. The the new QA engineer. Where where are we with that?
Javier: We have a strong pipeline. We we have three candidates who are are in the final-round interview loop.
Sarah: Good. Good. Are are they are they strong? What's what's the the profile you're looking for?
Javier: The the job description we we workshopped with HR is is what we are screening for. We we need an 'SDET'. A a Software Development Engineer in Test. Not not a manual tester. This person this person must be be ableto write code. They they must be able to to contribute to the API automation suite from from day one.
Sarah: And and these candidates, they they have that?
Javier: Two of them do. They they have strong Java and Javascript experience. The third candidate is is more of a a manual tester, but but has good domain experience in in e-commerce.
Sarah: I I would lean toward the the SDETs. We we can teach someone e-commerce. We we can't we can't teach them how to be be a a software engineer.
Javier: I am in agreement. My my preference is for the two the two technical candidates. We are we are conducting their final panel interviews this week. I I am hopeful we will have a a primary candidate to present to you by by next Monday.
Sarah: That's great. That that's really good news. We we need the help. I I know your team is stretched.
Javier: We are we are managing the workload through through careful prioritization. But yes, the the new hire will be be important for our Q4 goals.
Sarah: Okay. Good. Keep keep me posted on that.
Javier: Will do.
Sarah: Alright, so the the last thing on my agenda was was you. Your your development. We we haven't really talked about about your career goals in in a while. What what are you what are you thinking about?
Javier: My my goals? I I am focused on the the team's objectives. The the automation roadmap. The the shift-left initiative. These are these are my primary focus.
Sarah: I I get that. And and I appreciate how how focused you are on the the execution. But but what about your growth? Where where do you want to be in in two years? Or or five? Still still leading this team? Or or something else?
Javier: I I enjoy the technical challenges of quality assurance. The the field is it's always evolving. There are there are new tools, new new methodologies. I I am interested in in perhaps getting a a certification in in cloud security. Or or perhaps performance engineering.
Sarah: Okay. So so deeper deeper technical expertise. Not not necessarily managing a a bigger team or or managing managers?
Javier: I I am I am comfortable with my current team size. It it allows me to stay 'hands-on'. I I can still review code. I I can still contribute to the automation framework. If if my team grew much larger, I I would become just just a people manager. I I would I would lose my my technical edge.
Sarah: I I see. That's that's fair. A a lot of people feel that way. They they want to stay on that that technical track.
Javier: Yes. I I believe my my primary value to the the organization is is my technical and and architectural expertise in in quality.
Sarah: Okay. Well, let's let's support that then. If if you want to to pursue a a security certification, we we can definitely get get budget for that. Why why don't you why don't you look into some programs? Find find one you you think is valuable, and and send it to me. We we can get it approved.
Javier: Thank you, Sarah. That that is very supportive. I I will research the the top-tier certifications and and send you a a proposal with with the costs and the the time commitment.
Sarah: Sounds good. Okay, well I I think that's everything on my list. We we talked about the metrics. We we talked about automation and and the roadmap. We we did the RCA on the the checkout bug. And and hiring and and your development. That's that's a lot.
Javier: It was a productive agenda.
Sarah: It was. So action items. You're you're going to send me the the Confluence link for the the automation roadmap. And and you're going to send me the the proposal for the the certification.
Javier: Correct. I will have those to you by by end of day tomorrow.
Sarah: Perfect. And and I will I will follow up on the the budget for that. And and keep me in the loop on those those final two candidates.
Javier: Will do. You will be you will be in the the debrief meeting.
Sarah: Great. Alright, Javier. Thanks thanks for the time. This was this was good.
Javier: Thank you, Sarah. Have a productive week.
Sarah: You too. Bye.
Javier: Goodbye.
