Code Review Weekly Workshop - Sep 16, 2022

[Speaker 1]
Recording. Thanks for joining us for the Code Review weekly workshop. Yannick's got something to show and tell and or a question.

So what's going on, Yannick?

[Speaker 2]
Yeah. Hey, everybody. Probably I've got all of the just mentioned a little bit.

What I'm bringing along today is in our last meetings we've been speaking about merge requests that include GraphQL queries and that they are a little bit tricky to test and review and all these sorts of things. The DMR we're looking at here is a community contribution. In my eyes, an astonishingly good one.

And on one hand, the changes are pretty much straightforward, we are having an already existing feature, which is using the REST API and it is now refactored to be using GraphQL queries. No more features added, just a refactoring, pretty straightforward. So that's fantastic.

I reviewed this the way I reviewed things, which I'll be telling you more about in a second. Approved it, gave it to another maintainer because without any comments, I was just like, okay, this is how you do this. This is just perfect.

Turns out not quite. So there were other people involved and I was very happy to have them around finding no major things, but still a couple of them. So I think this is something worth mentioning or worth bringing to this meeting.

We could now approach this in a couple of ways and whatever you feel comfortable with. We could option one, just start out giving this a review without looking at all the things others have found and see how far we can take it. Or we just basically can kind of expand all the threats and see what is happening there.

[Speaker 1]
I'm really interested in your thought process. What were you looking at where you concluded this was okay? And then what would be the lessons learned after?

Sounds like you've piqued my interest. The story sounded like just every other day, but then darkness crept into the land. And I got to hear the rest of the story.

[Speaker 2]
I'm very, very happy to do this this way. Full disclosure, the things that are mentioned in here, I have not yet fully understood, which is probably a part of the thing that why I didn't mention them. But I'm happy to tell everybody how I approach this.

So as a quick overview, we are looking at the issue analytics request for projects and groups, if I remember. So it is pretty much straightforward, if we're getting the data, displaying it, GraphQL, kind of the same, no mutations, nothing crazy, pretty much getting data, showing it. So I kind of deal with this.

First thing I did, manual testing, everything's looking fine. All the features are there. As far as I could tell, this has been working.

So therefore, that was pretty, pretty good. Checking that, that would be my very first go-to. My second go-to would probably be the tests.

And therefore, and that is also kind of probably the trap I fell into. I've been looking at the tests and checking for what changed. Is there anything missing?

Is there anything significant going in there? And what I definitely first encountered is the test suite really didn't change that much. Other than obviously, we've been feeding a get request into it.

And now we're mocking all the GraphQL things and these kinds of things. But other than that, nothing has really changed. In terms of functionality, there still was a full box of all the things that we had before.

So I was kind of already happy with that. But I see that no tests are failing, no tests have been removed because I was not convinced that a GraphQL refactoring requires any more testing that we already be having within our REST implementation. That was my bold statement.

So having the test suite covered, I was basically just looking for the actual implementation, if I can see anything that makes me raise an eyebrow. But it was most of the things were kind of this query or basically implementation codes to actually trigger the query, keys that have been renamed, things like that. Nothing to really pay too close of an attention to.

So that was kind of my first approach. Anything that makes you raise an eyebrow?

[Speaker 1]
Well, I got to hear what happens next. You said you won't believe what happens next. And I got to hear what happens.

[Speaker 2]
Okay. So yeah, that is hence my first comment. I was sold on this thing.

Okay. Nice. Great contribution.

Okay. Great contribution. Very impressed.

All good. The fantastic Dave then took over. And here is what I feel like, okay, I need to work on my review skills here because I don't like this.

Minor suggestion. Well, okay. Praise.

Yeah, I probably should have done this as well. But other than that, a potential testing gap. Let's see what Dave has to say.

And this is the part where I don't fully understand what's going on as well. Should we also test for type project and check that the query gets past the correct values? I think we can retrieve the variables by doing something like wrapper and don't worry about this too much now, but help me out.

What's going on with this potential or realistic or actual test gap?

[Speaker 1]
So going to the, this is likely the way I'm going to interpret this situation. And this is a lesson learned for whether you're front-end or back-end. This is something to just keep in mind.

So the intent here would have been pure refactor. No new features. We used to do rest.

Now we're doing GraphQL and this component, pure refactor. It's a big assumption that we have proper coverage to start with. And so that's probably likely what Dave is catching on to.

And so one way to test, so you can either look at the MR as an absolute, what is the health of our code? Absolutely. And what's the health of the code Delta because of this MR. And if you just look absolutely, it'd be, okay, I'm touching these lines. Let me just start removing lines and do tests fail. Like that's my favourite way to test if there's code coverage or not. And I'd likely Dave, Dave might've, you know, that's rather than looking through the tests, like you kind of described, that's maybe what Dave did was, okay, we're doing some sort of like conditions variables, which is a little different.

So let me just check if we've actually tested them by removing them and running the tests and seeing if they fail. That's, that's how I know that I might've. And I've at times stumbled across a reduction in coverage, but then it brings up the big question of, is that like a blocking issue?

Because did we have this coverage before is it in scope kind of, right. But I would say, I would argue we're touching it. So whatever we've touched and changed should be well tested because we want to ensure that a refactor is good and working and like as expected and continues to.

So I would, I think refactoring does imply if there's no test for something, you get to, you get to add tests now.

[Speaker 2]
I personally feel since this is a community contribution that I'd probably be fine without them, but that is a night long discussion we can, we can have about this and there's more than one opinion. So that's, that's absolutely fine.

[Speaker 1]
But, and I think there's different levels of it too, but I'd love to hear Cassio or Drew or Andre, what'd you think on this situation of a refactoring and identifying pre-existing testing gaps? How would you handle that in a review?

[Speaker 5]
So for my perspective, it's basically depends right on many factors. One of which is what Yannick mentioned versus community contribution versus the GitLab employee contribution. In general, what I try to follow is the voice coach rule.

So you should leave the code base or whatever you are touching in a better shape than it used to be. So definitely if there is the way to improve things that is not very cumbersome to still do as a part of the same merge request, then we should do it.

[Speaker 2]
I'm breathing heavily. Sorry.

[Speaker 3]
Um, I reckon like when you say it's a community contribution, this is a contribution from someone from the community, not from GitLab, yeah?

[Speaker 1]
Yeah.

[Speaker 3]
Okay. And so they touched on the code that doesn't have a test coverage.

[Speaker 1]
Yes. It has, it has sound level. It's like, sounds like there's just some holes in it.

[Speaker 3]
Right. Okay. I reckon, I guess, um, for, for GitLab people to when, when they touch on the code and there is no test coverage, I guess, uh, there is a bit of a conversation.

I mean, sorry, I I'm very new to GitLab, just a disclaimer.

[Speaker 2]
Yeah. No worries. No worries.

[Speaker 3]
Yeah. So generally speaking, in my opinion, I should say, um, like if someone has, if it's a very crucial path and someone has the time to add the test and then definitely we should definitely encourage people to add tests. But if it's not a very crucial path and, and the deadline is very short, then, uh, generally speaking, I would still say we should add the test, but I guess then we can kind of let it slide to some degree and, uh, hoping that the person would come back when they have more time to add the tests.

But, uh, for like a community, um, a contribution, I would say, I, I don't know why, but I, I feel like I would be a little bit stricter. Like they have all the time. I think we don't pay for that time.

So they have all the time in world. They want to contribute, so they should contribute something good.

[Speaker 1]
That is such a good perspective. Love it. Uh, that's so funny.

Um, I had not, that's, that's usually not the perspective I have on community contributions. I'm usually it's like, you've donated your time. Thank you so much.

Like, I don't want to ask for more, but I love your perspective of like, what else are you doing? I'm just joking. That's really, that's, I think that that's, I think both, both of you touched on, um, you do have to apply discretion here.

So you can't be over prescriptive because some things are like, Hey, things are broken on.com. And this is a very fragile piece of code that we know fix this, but we can't add a test because that's too complex. We'd really like to fix.com right now. Like that's a big deal for, yeah, we should probably do that and defer figuring out how we can add regression testing if that comes up. Um, so those, those trade-offs decisions definitely need to happen, uh, at times. Um, I will say when something isn't urgent with MRs, uh, it's, it's worth a cycle of just keeping that standard of quality.

Uh, as, as high as we can, it's worth one MR cycle, like, and, and this case, I think David catching that, like, that's great. Yeah. That's, that was a really good catch.

And, um, I'm glad Natalia jumped in and like, yeah, we don't want to use wrapper VM. So Natalia was able to, like, this looks like awesome asynchronous collaboration. David identified a testing gap.

Natalia had a great idea for a better way to check it out. Uh, and, um, what David did then is this is my number one way, favourite way to help contributors. If we can provide a patch for them, uh, where they can, they could basically review and study it and apply it.

Um, every time I've applied a patch, it's really easy to think like, oh, well, no one's learning anything or whatever. Like I've heard, you know, some people comment about patches in that way, but, uh, I think like well over 99% of the time I provide a patch, uh, the person that applies it, you know, they're, they're now reviewing and looking over it and owning the solution. And, uh, that's a great way to communicate, um, changes and makes it easy for the contributor.

So the way David applied a patch was awesome. Um, yeah, a hundred percent.

[Speaker 2]
Um, regarding the, um, we're going to catch this.

[Speaker 1]
Cassia, I see your, your hand is up.

[Speaker 3]
Uh, I just wanted to ask, um, by patch, do you mean someone from GitLab jumps in and basically add some code on top of the contributor? Is that a patch?

[Speaker 1]
That's a really good question. So it's this comment that David left, which is this big code block, which is actually a diff. So you can actually hover over this code block and copy it onto the clipboard.

Uh, sorry, can you go back to, can you go back to, uh, yep. Uh, and so what the contributor do is actually copy this to the clipboard and then through the terminal, do something like PB paste, pipe, get apply, and it will apply the patch locally. So the change is still coming from the contributor, but the reviewer and the maintainer it's kind of just giving a, is giving a, uh, large, not large, but like, it's not a trivial code suggestion.

It's just like, here's a code suggestion. And, uh, so yeah, using patches, I highly recommend, um, getting familiar with generating patches and applying patches as we review code. Um, this is a, this is, I, I do it because I, I could spend an hour trying to word something just right.

Or I could spend five minutes of just this, here's the code I'm suggesting. What do you think? And now we're just collaborating like on the code and that's great.

[Speaker 5]
Isn't it actually something that is already integrated into this, uh, review flow in GitLab?

[Speaker 1]
The suggestion there?

[Speaker 5]
Yeah.

[Speaker 1]
Yeah. Um, that's a good observation.

[Speaker 2]
Um, be a little careful with those, like they work fantastic for small changes, like one-liners and stuff. But if things get too complex, it is at least a little painful to use. Um, and it could, you could easily introduce things that you, there was no intention of introducing.

So that is what to keep in mind.

[Speaker 1]
It's, um, they, the suggestions don't work under the hood is not get patches working under the hood. It's a, it's a range of lines and then code we're going to replace on top of that range. And so that's, they're, they're somewhat limited in that sense for one-liners like this.

Easy, easy, easy. And that's great. Um, I actually ended up just sending patches all the time because I like patches so much and I'm a little bitter that we didn't use patches for suggested changes.

So.

[Speaker 2]
And you can have easily one patch for changes and multiple files and things like that. So, um, it is regarding the context. Um, it has a lot of benefits.

Yeah.

[Speaker 1]
But you did identify a great feature request. Wow. That was, I'm going to pretend that didn't come from me so that I could say we got, you know, lots of people suggesting this.

[Speaker 2]
Multiple file patches or suggestions.

[Speaker 1]
Suggestions that actually support the patch syntax would be amazing.

[Speaker 5]
Yeah. That would be fantastic.

[Speaker 1]
I do agree. Yeah. Wow.

It's three people that both think that that's great. I'm trying to, I'm going to create an issue and get all the thumbs up on it. That's how I guess we prioritise.

Thanks. All right.

[Speaker 4]
I've been on the contributor side of this problem. Like when I'm refactoring something somewhere and verify and I'll come across the test gap, I almost always send the new test coverage in a separate merge request because the reviews are so much faster. Like it's really easy to review one test added to a section of code.

And it's also way easier to review a refactor when it is a pure refactor. And I can like link out to the tests that say, here's why this refactor is bulletproof. These are all of our outcomes and I'm not touching them.

So, you know, the refactor is good. Both of those reviews are like lightning. They're great.

So I highly, like I haven't had this with a contributor yet, but if I did, I might even encourage them to do that because I think the diffs in the conversations are super clean that way.

[Speaker 1]
Yeah, that's a good, that's a really good option to consider too. And for community contributions, I've often, I'll create the followup MR too of like, okay, there's a big testing thing that I'm about to make a suggestion for. I don't want to ask more of them.

I know this will take me maybe five to 10 minutes. I'm about to write the code for a patch anyways. I'm just going to just write the code in a separate branch and create an MR for it.

And that's, I think that is definitely a good option too. I call these fast followups. And if it has to be, if we identify testing gap, ideally we could follow up on a right away.

It's not a deferred 46,000 drops in the bucket followup. It would be hopefully a faster followup. That's a good point.

[Speaker 2]
Yeah. Thanks so much for the input folks. I feel a little better about this now and I'm starting to realise that my take on this, and please take it with a grain of salt, because what I'm about to say is heavily opinionated and I absolutely might be wrong about this.

And also I do think that this is not a binary question, but regarding a community contribution, I kind of feel like I heavily agree with the followup issue and this should be set. And I'm super happy with the way this has been handled in this MR with basically really actively helping out the contributor. But I would also say if our code base already has a testing gap, that's kind of our issue.

And if they are dealing with what they have and solving it within these boundaries, then to me that kind of implies as good enough. It's still definitely requires a followup issue and it is something we should be taking care of. But regarding a blocking comment or basically having just enough to be blocking, I'm not so sure about this.

And therefore I also, that's why I started to breathe heavily when I hear the Boy Scout rule term. And that is maybe because in my past career, I noticed that this thing was kind of also being misused as a bit of a feature creep as well. So I'm super happy with getting our code base polished, but let's be very sure about the scope we're currently speaking about.

And if we encounter any further problems or additional problems, happy to fix them. Let's keep our MR small and let's open another issue. Let's do all of these things.

That's kind of what I would tend to decide for. But as said, there's more than one opinion on this.

[Speaker 1]
Well, I think that it does depend because if you are refactoring, tests allow us to make changes with confidence. Depending on the testing gap, sometimes you do have contributors just stumble upon, this is actually really fragile. And so we really, for us to actually change this, we need to have a stricter testing harness almost first before we change it because of all the context and stuff going on.

So I do agree. You're putting weight on prioritising more than just we got to get everything absolutely perfect. But I do think even if we want to keep MR small, we do.

Testing is really good at also just asserting the current change. And if there hasn't been a second maintainer MR cycle, like if this is the first maintainer MR cycle, if someone catches that, like, that's great. If this is like the third cycle and someone's introducing new things, that's Yeah, I would agree.

That's a little maybe a little nitpicky. But if this is the first one, it sounds sounds good catch to me. I don't know.

But it's okay if we don't agree on it. We can still work together sometimes. As long as you pretend to agree.

Kasia had a great question. Do you want to voice your question, Kasia?

[Speaker 3]
I guess it was kind of a follow up to what you were saying that most of the time you would just create a follow up PR with the test coverage. I was just wondering what would be our preference then? Is the preference of GitLab, okay, let the PR merge by the contributor and then we follow up?

Like, what if we don't have the time or what we don't have the capacity, then it will be there will be still a whole vulnerability, potentially.

[Speaker 1]
Yeah, you ask a great, great question. And I would say the probably the preference would be we handle it in this MR. But I think what Janik is and others are pointing at is, you do have to kind of judge the fatigue level of the MR and the fatigue level of the contributor. And if this MR has been stretched out, things that aren't critical, we don't want to just continue to add fatigue on everyone involved in the MR. We want people to feel efficient with the way MRs are being handled on things. So we don't want things to hang around for a long time. So I think the preference would be we handle things in the MR that we identify them unless it's just very, very out of scope. But with community contributions especially, just reading the contributor and reading where how long this is stretched out, that can play a huge factor in whether we decide to follow up ourselves, create a follow-up issue.

And there's lots of competing goals. Because one, we want to fill the test gap, but or we could be inspired to create a follow-up issue that another community contributor can pick up. And we do want to encourage more community contributions.

And some community contributors love like, oh, I don't really know what to do. But now I'm really familiar with this change. I just made this.

I'd love to follow up with another one because I like making more changes. And they'll like doing that if MRs aren't so fatiguing. And so there's a lot when it comes to community contributions, there's a lot of soft things to kind of just read in the room.

And I don't think you can be over prescriptive about it. But if you're really not certain, I would highly recommend just reaching out and asking a question like David did of here's just a question. What do we think?

And I love how he labelled the question. He didn't label it. That was really appropriate for what this was of like, we're just asking a question.

If it wasn't going to work out, then it's not going to work out. Because it doesn't seem like it was super critical.

[Speaker 2]
I do agree. Awesome. Yeah.

Thanks, everybody, for sharing your thoughts on this. This was super insightful. That was basically all I had to say.

[Speaker 1]
I feel like you click baited me because you said like you won't believe what happens next. And I was like, I was like, picturing what's going to happen next and small testing gap was not what I was anticipating.

[Speaker 2]
Okay. Okay. Okay.

Maybe the term test gap is triggering me more than you. I kind of panic a little bit whenever I hear it.

[Speaker 1]
Oh, wow. Okay. How long have you coded on the GitLab project?

I'm joking. I'm doing a slight jab at the GitLab project. Cool.

All right. I have a small one to share. Let me get ready to share my screen by closing out some other things.

Here we go. Oh, man. All right.

Let's do it. So, this is actually the one we started pairing on last week. And so, it got merged this week, which is great.

This is an MR on the GitLab UI project. And the main takeaway for here I wanted to highlight was we identified the original set of changes. And it was hard to tell why we were making this change until in the comment below, we saw a link to an issue.

And when we linked to the issue, there was pictures of the actual bug that we were trying to change with this that shows up the GitLab project with the upstream GitLab UI project. But the original state of it didn't really have so, in the GitLab UI project, we have these stories for our components where we'll actually visually screenshot the rendering of the component. We'll do visual regression tests at this level, which is great.

But the changes weren't actually, like, testing these things. So, it wasn't clear if, like, we were actually fixing the bug. So, like, the changes to our story.

And it was really confusing what kind of why we were making the changes to the story. And so, I left some questions. One of the questions being here we were doing this.

This is a view this is a view behaviour feature that we don't use a whole lot of where we can use this dot sync modifier. And that was strange for me to see. So, I just left a question about it.

And then seeing some other things, I was just, like, these are a little not what I would expect. So, I left some questions. But my biggest question was where was it?

I can't find it. Oh, yes. Yes.


